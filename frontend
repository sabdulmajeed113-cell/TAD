"""
Streamlit frontend for "Personal Finance Chatbot"
Supports two backends (selectable in the UI):
  - IBM (example: IBM "3.2-2B-Instruct" style endpoint)
  - Hugging Face Inference API

How this file is organized:
  - Streamlit UI: user profile (student / professional), conversation interface, quick actions (budget summary, spending insights)
  - Prompt templates that adapt tone & complexity based on demographic
  - Backend wrapper functions: `call_ibm_model` and `call_huggingface_model` (both use RESTful calls; fill in your endpoint & keys)
  - Lightweight local caching of conversation for context

Required environment variables (export these in your terminal or set in your deployment):
  - IBM_API_URL     => URL of the IBM model endpoint (e.g. https://api.us-south.generative-ibm.example/v1)
  - IBM_API_KEY     => API key for IBM
  - HUGGINGFACE_TOKEN => Hugging Face Inference API token (if using HF)

Install requirements:
  pip install streamlit requests python-dotenv

Run:
  streamlit run app.py

Note: This is a frontend stub intended for demonstration and local testing. Replace the placeholder request/response handling with the exact SDK or REST contract required by your IBM model deployment.
"""

import os
import time
import json
from typing import List, Dict, Any

import streamlit as st
import requests
from dotenv import load_dotenv

load_dotenv()

# Configuration / env
IBM_API_URL = os.getenv("IBM_API_URL", "")
IBM_API_KEY = os.getenv("IBM_API_KEY", "")
HF_TOKEN = os.getenv("HUGGINGFACE_TOKEN", "")

# UI constants
APP_TITLE = "Personal Finance Chatbot (Demo)"
DEFAULT_MODEL_SELECTION = "ibm-3.2-2b-instruct"
AVAILABLE_BACKENDS = {
    "IBM (3.2-2B-Instruct)": "ibm-3.2-2b-instruct",
    "HuggingFace (inference)": "huggingface-inference"
}

# --------------------- Prompt templates ---------------------
PROMPT_BASE = (
    "You are a friendly, accurate and concise personal finance assistant. "
    "You must give practical, easy-to-follow advice and clearly state any assumptions."
)

PROMPT_FOR_ACTIONS = {
    "budget_summary": (
        "Given the user's income and expenses, produce a short, labeled budget summary: "
        "monthly income, essential expenses, discretionary spending, recommended savings, and 3 short tips to reduce costs."
    ),
    "spending_insights": (
        "Analyze the following spending list and return 3 actionable insights to optimize expenses, "
        "including one quick experiment the user can try this month."
    )
}

DEMOGRAPHIC_INSTRUCTIONS = {
    "student": "Use simple language, examples relevant to students (e.g., tuition, part-time income), and give low-cost saving tips.",
    "professional": "Use professional tone with concise recommendations and consider career-related expenses (e.g., commuting, subscriptions)."
}

# --------------------- Backend wrappers ---------------------

def call_ibm_model(prompt: str, model: str = "ibm-3.2-2b-instruct", temperature: float = 0.2) -> str:
    """
    Placeholder REST call to IBM Generative model. Replace body and headers to match IBM's actual endpoint specification.
    Expects IBM_API_URL and IBM_API_KEY to be set in env.
    """
    if not IBM_API_URL or not IBM_API_KEY:
        return "ERROR: IBM_API_URL or IBM_API_KEY not configured."

    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {IBM_API_KEY}"
    }

    body = {
        "model": model,
        "input": prompt,
        "temperature": temperature,
        # other fields depend on IBM API
    }

    try:
        resp = requests.post(IBM_API_URL, headers=headers, json=body, timeout=20)
        resp.raise_for_status()
        data = resp.json()
        # The exact path to the generated text will depend on IBM's JSON structure. This is a best-effort.
        # Try common keys used by LLM style APIs:
        if isinstance(data, dict):
            # search likely fields
            for key in ["output", "result", "text", "generated_text", "choices"]:
                if key in data:
                    val = data[key]
                    if isinstance(val, list) and val:
                        # choices-like structure
                        first = val[0]
                        if isinstance(first, dict) and "text" in first:
                            return first["text"].strip()
                        return str(val[0])
                    if isinstance(val, str):
                        return val.strip()
        # Fallback: return full JSON
        return json.dumps(data, indent=2)
    except Exception as e:
        return f"ERROR calling IBM API: {e}"


def call_huggingface_model(prompt: str, model: str = "gpt2", max_length: int = 512) -> str:
    """
    Call Hugging Face Inference API. Requires HUGGINGFACE_TOKEN in env.
    """
    if not HF_TOKEN:
        return "ERROR: HUGGINGFACE_TOKEN not set in environment."

    HF_URL = f"https://api-inference.huggingface.co/models/{model}"
    headers = {
        "Authorization": f"Bearer {HF_TOKEN}",
        "Content-Type": "application/json"
    }
    payload = {"inputs": prompt, "parameters": {"max_new_tokens": max_length}}
    try:
        res = requests.post(HF_URL, headers=headers, json=payload, timeout=30)
        res.raise_for_status()
        out = res.json()
        # HF returns list of dicts for text-generation models
        if isinstance(out, list) and out and isinstance(out[0], dict) and "generated_text" in out[0]:
            return out[0]["generated_text"].strip()
        if isinstance(out, dict) and "generated_text" in out:
            return out["generated_text"].strip()
        return json.dumps(out, indent=2)
    except Exception as e:
        return f"ERROR calling HuggingFace API: {e}"


# --------------------- Prompt builder ---------------------

def build_prompt(user_profile: Dict[str, Any], user_message: str, system_instructions: str = "") -> str:
    """Compose a prompt using the base instructions, demographic instructions, and the user message."""
    demographic = user_profile.get("demographic", "professional")
    extra = DEMOGRAPHIC_INSTRUCTIONS.get(demographic, "")

    profile_snippet = []
    if user_profile.get("age"):
        profile_snippet.append(f"age: {user_profile.get('age')}")
    if user_profile.get("income"):
        profile_snippet.append(f"monthly_income: {user_profile.get('income')}")
    if user_profile.get("location"):
        profile_snippet.append(f"location: {user_profile.get('location')}")

    profile_text = "; ".join(profile_snippet)

    prompt_parts = [PROMPT_BASE]
    if extra:
        prompt_parts.append(extra)
    if system_instructions:
        prompt_parts.append(system_instructions)
    if profile_text:
        prompt_parts.append(f"User profile: {profile_text}.")

    prompt_parts.append("User message:\n" + user_message)

    return "\n\n".join(prompt_parts)


# --------------------- Streamlit App ---------------------

st.set_page_config(page_title=APP_TITLE, layout="wide")

st.title(APP_TITLE)

# Sidebar: backend + user profile
with st.sidebar:
    st.header("Settings")
    backend = st.selectbox("Choose model backend", options=list(AVAILABLE_BACKENDS.keys()), index=0)
    model_key = AVAILABLE_BACKENDS[backend]

    st.subheader("User profile")
    demographic = st.radio("I'm a", options=["student", "professional"], index=1)
    age = st.number_input("Age (optional)", min_value=13, max_value=100, value=25)
    location = st.text_input("Location (optional)")
    monthly_income = st.number_input("Monthly income (optional)", min_value=0, value=0)

    st.markdown("---")
    st.write("Environment checks")
    st.write(f"IBM configured: {'yes' if IBM_API_URL and IBM_API_KEY else 'no'}")
    st.write(f"HuggingFace token: {'yes' if HF_TOKEN else 'no'}")
    st.markdown("---")
    if st.button("Clear conversation"):
        if "conversation" in st.session_state:
            del st.session_state["conversation"]
            st.experimental_rerun()

# Initialize conversation state
if "conversation" not in st.session_state:
    st.session_state["conversation"] = []  # List of tuples (role, text)

# Layout: left chat + right quick-actions
left, right = st.columns([2, 1])

with left:
    st.subheader("Chat")

    # Show conversation
    for role, text in st.session_state["conversation"]:
        if role == "user":
            st.markdown(f"**You:** {text}")
        else:
            st.markdown(f"**Assistant:** {text}")

    user_input = st.text_area("Your message", height=120, key="user_input")

    col1, col2 = st.columns([1, 1])
    with col1:
        if st.button("Send"):
            if user_input.strip() == "":
                st.warning("Please type a message.")
            else:
                # Append user message
                st.session_state["conversation"].append(("user", user_input))

                user_profile = {
                    "demographic": demographic,
                    "age": age,
                    "location": location,
                    "income": monthly_income,
                }

                system_instructions = "Be concise and provide numbered steps when giving action items."
                prompt = build_prompt(user_profile, user_input, system_instructions)

                with st.spinner("Contacting model..."):
                    if model_key.startswith("ibm"):
                        answer = call_ibm_model(prompt, model=model_key)
                    else:
                        # Use a safe default HF model name; in production let the user choose
                        hf_model = "google/flan-t5-large" if HF_TOKEN else "gpt2"
                        answer = call_huggingface_model(prompt, model=hf_model)

                st.session_state["conversation"].append(("assistant", answer))
                st.experimental_rerun()

    with col2:
        if st.button("Regenerate last reply"):
            # Regenerate the last assistant answer
            # Build prompt again using last user message
            last_user = None
            for r, t in reversed(st.session_state["conversation"]):
                if r == "user":
                    last_user = t
                    break
            if not last_user:
                st.warning("No user message to regenerate from.")
            else:
                user_profile = {
                    "demographic": demographic,
                    "age": age,
                    "location": location,
                    "income": monthly_income,
                }
                prompt = build_prompt(user_profile, last_user, "Regenerate with slightly more details.")
                with st.spinner("Regenerating..."):
                    if model_key.startswith("ibm"):
                        answer = call_ibm_model(prompt, model=model_key)
                    else:
                        hf_model = "google/flan-t5-large" if HF_TOKEN else "gpt2"
                        answer = call_huggingface_model(prompt, model=hf_model)
                st.session_state["conversation"].append(("assistant", answer))
                st.experimental_rerun()

with right:
    st.subheader("Quick actions")
    st.write("Use these to generate structured outputs from your profile and a short input.")

    sample_expander = st.expander("Sample inputs")
    with sample_expander:
        st.markdown("- `I need a monthly budget for income 40000 and expenses: rent 12000, food 8000, transport 2000, subscriptions 2000.`")
        st.markdown("- `How can I save more for retirement?`")

    # Budget summary generator
    st.markdown("### Budget summary generator")
    sample_budget = st.text_area("Paste income & expenses (short)", height=120, key="budget_input")
    if st.button("Generate budget summary"):
        if not sample_budget.strip():
            st.warning("Enter income & expenses first")
        else:
            user_profile = {"demographic": demographic, "age": age, "location": location, "income": monthly_income}
            system_inst = PROMPT_FOR_ACTIONS["budget_summary"]
            prompt = build_prompt(user_profile, sample_budget, system_inst)
            with st.spinner("Generating budget summary..."):
                if model_key.startswith("ibm"):
                    out = call_ibm_model(prompt, model=model_key)
                else:
                    hf_model = "google/flan-t5-large" if HF_TOKEN else "gpt2"
                    out = call_huggingface_model(prompt, model=hf_model)
            st.text_area("Budget Summary", value=out, height=240)

    st.markdown("---")
    st.markdown("### Spending insights")
    sample_spend = st.text_area("Paste recent transactions (one per line, amount, category)", height=120, key="spend_input")
    if st.button("Analyze spending"):
        if not sample_spend.strip():
            st.warning("Enter transactions first")
        else:
            user_profile = {"demographic": demographic, "age": age, "location": location, "income": monthly_income}
            system_inst = PROMPT_FOR_ACTIONS["spending_insights"]
            prompt = build_prompt(user_profile, sample_spend, system_inst)
            with st.spinner("Analyzing spending..."):
                if model_key.startswith("ibm"):
                    out = call_ibm_model(prompt, model=model_key)
                else:
                    hf_model = "google/flan-t5-large" if HF_TOKEN else "gpt2"
                    out = call_huggingface_model(prompt, model=hf_model)
            st.text_area("Spending Insights", value=out, height=240)

    st.markdown("---")
    st.write("Developer notes:")
    st.write("• Replace request payload and response parsing in `call_ibm_model` to match your IBM deployment's contract.")
    st.write("• Add authentication helper if your infra requires token refresh or OAuth.")

# Footer
st.markdown("---")
st.caption("This is a demo frontend — secure keys and server-side orchestration are recommended for production deployments.")

